John, Stuart,

Here are all the sources of error I can think of in the Teachspin experiment.

(1) Decay offset due to different natures of stopping and decaying events.
The light from the stopping event is generated BEFORE the muon stops, while it is still moving.  Since the pulse detector works on the leading edge of both, that means it starts counting the lifetime too soon.  If the muon is still moving, there may be some SR time dilation that would lengthen its lifetime.
The light from the decaying event is measured AFTER the muon decays and is mostly generated by the electron produced.
NET EFFECT: Should make measured lifetimes longer than actual ones.  Rough order of magnitude would be the width of the stopping pulse.

(2) Error due to variability in pulse size/shape. (2a stopping pulse, 2b decay pulse)
Since the discriminator circuit is tuned to a specific voltage level, and the leading edge has finite slope, and the pulse shape may vary, and the size of the pulse may vary, we would expect a larger pulse to trigger slightly sooner than a smaller pulse.
NET EFFECT: Random errors in the time that the pulse is detected.  Min to max range would be somewhat less than half the typical pulse width (soonest would be the point where the largest pulse triggers, latest would be near the middle (top) of a small pulse just barely able to trigger).

(3) Metastability of discriminator. (3a stopping pulse, 3b decay pulse)
Any time a circuit has to take an analog input and make a digital decision on it, there will be signals very close to the decision threshold that can cause the circuit to hang in a metastable intermediate state.  (This problem occurs most famously with flip-flops receiving data asynchronously to their clocks, but it can be caused by thresholds in voltage as well as in time.)  There is no theoretical upper bound to how long this hanging can occur; however, the probability of continued hanging decays exponentially with time (it is an unstable equilibrium like a pencil balancing on its point).  The exact behavior depends on circuit construction; a good circuit can make this problem very unlikely, at the cost of taking more time to make the decision.  I don't know yet what the details of the Teachspin discriminator are.
NET EFFECT: Until analyzed more closely, we should assume that there may be occasional pulses which are effectively delayed by an entire clock cycle, or even (much more rarely) two.  Since this can happen on either pulse, it could lengthen or shrink the measured decay time.  Large and small pulses may behave differently; if there is a difference in pulse sizes between stopping and decay pulses, we could get a net average offset, but if not the mean of this should be zero.

(4) Digitization error of pulse time. (4a stopping pulse, 4b decay pulse)
I assume that the stopping pulse is digitized to synchronize with a clock pulse that is somewhere between 1 nS and 20 nS (the bin size) cycle time since the file data is reported in multiples of 20 nS.
NET EFFECT: This adds a random error which is roughly uniformly distributed over the cycle time, giving a systematic offset of perhaps half the cycle time.
SOLUTION: For (4a), see solution to (5) below.  For (4b), to zeroth order we can treat all decays in a bin as occurring at the center of the bin.  The first order correction will move this slightly earlier since the slope is monotonic negative.  The second order and all higher even order corrections should be zero by symmetry.  The 3rd and all higher odd order corrections will also move the mean decay time slightly earlier.  This series should be simple enough to solve exactly, but I haven't looked into that yet.
IMPLEMENTATION: I currently use the zeroth order correction only (all decays are assumed to be at the center of their bin, i.e. 10 nS after the bin start for 20 nS bins).

(5) Problems with detecting very fast decays.
Very fast decays may be difficult or impossible to detect.  Since the scintillator takes nanoseconds to emit all its light, two pulses starting less than a few nanoseconds apart may appear to be one wide pulse.  There may also be issues throughout the analog circuitry, from the PMT to the discriminator; each may require some time after emitting a pulse to recover and be ready for another one.  The recording computer, USB interface, and software may also impose limits.  Examination of a large output file with over 100K decay events shows zero decays in the 0 nS and 20 nS bins, and a possibly reduced number of decays in the 40 nS bin.
NET EFFECT: Decays faster than some minimum time are lost.
SOLUTION: Fortunately, exponential decay is memoryless.  We can simply discard the first few bins to solve this problem.  Doing so also fixes most of (1) and all of (2a) (3a) (4a).  It also has the effect of synchronizing all the muon decay start times with the beginning of the first undiscarded bin.
IMPLEMENTATION: My analysis software currently discards all initial empty bins and the first non-empty bin.  For most files this means discarding the first 3 bins (all decays marked as 40 nS or less).  This loses about 0.1% of the data, which is acceptable.

(6) Clock jitter and drift
The clock driving the bin sampling is not perfect, but will have some jitter.  It may also drift slightly over time due to temperature, circuit aging, power supply variation, or other causes.  For any reasonably good crystal oscillator, this will probably be much smaller than other sources of error listed here and so negligible, but it is not zero.

(7) False decay noise due to separate transit events close together.
This is probably the largest and most serious noise problem in the experiment.
NET EFFECT: If we assume that the transits are random, then the first transit has a uniform probability over time and the second one has the same distribution, so the probability of a false decay event from two transits is proportional to the decay window size times the square of the transit probability.
SOLUTION: Given the power of 2, the easiest way to reduce this noise would be to reduce transit events.  Cutting them down by a factor of 10 would reduce the false decay from this cause by a factor of 100.  One approach of course is to use a muon beam with momentum tuned to mostly stop in the detector, i.e., to increase the number of true decay events relative to the false ones.  Staying with cosmogenic muons only, detector paddles above and below the main scintillator could give signals that would allow us to detect and filter out many transit events. 
It is also possible to estimate the mean number of false decays per bin and to "subtract that out" of the data in various ways.  However, this still leaves residual errors from (a) the error in estimating the mean, and (b) the variation about the mean.  An alternate statistical approach would be to leave in all the decay events (true and false), but estimate the probability that each one is true (which will depend on the bin) and weight them accordingly.  This is particularly simple in the Bayesian approach, where the log (relative) probability of the (true) event is simply added to the log (relative) likelihood of the hypothesis: we can just multiply that log event probability by the probability that the event is true and add that instead.



